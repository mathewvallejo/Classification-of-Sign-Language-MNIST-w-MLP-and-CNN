{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "## Acquire the data\n",
        "\n",
        "Here we load the Sign Language MNIST dataset for classification via Multilayer Perceptron (MLP) and Convolutional Neural Network (CNN).  The dataset is normalized and broken into traning, validation, and testing sets for use with the models."
      ],
      "metadata": {
        "id": "ei9LDE_W5Xr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download_datasets()\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"COMP_551_A3_Shareable.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1RxfnoXX-1MvlNM93d7GC-68PWB-QbOhL\n",
        "\"\"\"\n",
        "\n",
        "import gdown\n",
        "\n",
        "def download_datasets() -> None:\n",
        "\n",
        "    train_data_path = \"https://drive.google.com/uc?id=1Q6Iefx7rWFSpHhuQJ7ygPHHTTxMwLYth\"\n",
        "    test_data_path = \"https://drive.google.com/uc?id=1M6aIcBXmRKRR1go9kKpDA9aGq_Uy0mz0\"\n",
        "\n",
        "    output_train = 'sign_mnist_train.csv'\n",
        "    output_test = 'sign_mnist_test.csv'\n",
        "\n",
        "    gdown.download(train_data_path, output_train)\n",
        "    gdown.download(test_data_path, output_test)"
      ],
      "metadata": {
        "id": "GHygVyEqZfS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Path to dataset file in Google Drive\n",
        "train = '../data/sign_mnist_train.csv'\n",
        "test = '../data/sign_mnist_test.csv'\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "df_train = pd.read_csv(train)\n",
        "df_test = pd.read_csv(test)\n",
        "\n",
        "print(df_train.head())\n",
        "\n",
        "df_train.info()\n",
        "df_train.shape\n",
        "# df_train.describe()\n",
        "df_train.head(10)\n",
        "\n",
        "# Separate labels from dataset and create validation set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "y_train = df_train['label']\n",
        "x_train = df_train.drop(['label'], axis=1)\n",
        "\n",
        "y_test = df_test['label']\n",
        "x_test = df_test.drop(['label'], axis=1)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train = np.asarray(x_train)/255\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "x_val = np.asarray(x_val)/255\n",
        "y_val = np.asarray(y_val)\n",
        "\n",
        "x_test = np.asarray(x_test)/255\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"x_val shape:\", x_val.shape)\n",
        "\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "\n",
        "unique_labels_y = np.unique(y_train)\n",
        "print(unique_labels_y.min())\n",
        "print(unique_labels_y.max())\n",
        "\n",
        "num_classes_y = len(unique_labels_y) + 1\n",
        "print(\"Number of unique labels in y_train:\", num_classes_y)"
      ],
      "metadata": {
        "id": "hkC562PyaQey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "## Implement MLP\n",
        "\n",
        "Here we define a multilayer perceptron (MLP) with mini-batch gradient descent optimization for training the network. The framework comprises several classes representing different types of neural network layers. The NeuralNetLayer serves as the base class, defining common attributes like gradients, parameters, lambda coefficients for regularization, and flags for L2 penalty normalization. Subclasses include LinearLayer for fully connected layers with random weight initialization and L2 regularization, ReLULayer implementing the ReLU activation function, SoftmaxOutputLayer for the output layer with softmax activation in classification tasks, SigmoidLayer implementing the sigmoid activation function, and LeakyReLULayer for the Leaky ReLU activation function. Each layer class provides methods for forward propagation (forward) and backpropagation (backward).\n",
        "\n",
        "Additionally, the MiniBatchMLP class represents the multi-layer perceptron model and utilizes the forward and backward methods of individual layers to perform forward and backward passes through the network. It also includes functionality to calculate the number of hidden layers in the network and compute regularization based on L2 norms of parameters. The MiniBatchGradientDescentOptimizer class implements mini-batch gradient descent optimization for updating the network parameters. It uses a specified learning rate (lr) for parameter updates based on gradients computed during backpropagation, updating parameters for each layer based on the mean gradient of the mini-batch."
      ],
      "metadata": {
        "id": "4TtP1PcG6GJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the NeuralNetLayer base class and its subclasses\n",
        "class NeuralNetLayer:\n",
        "    def __init__(self):\n",
        "        self.gradient = None\n",
        "        self.parameters = None\n",
        "        self.lambda_coef = 0\n",
        "        self.normalize_l2_penalty = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class LinearLayer(NeuralNetLayer):\n",
        "    def __init__(self, input_size, output_size, lambda_coef=0, normalize_l2_penalty=False):\n",
        "        super().__init__()\n",
        "        self.ni = input_size\n",
        "        self.no = output_size\n",
        "        self.w = np.random.randn(output_size, input_size)\n",
        "        self.b = np.random.randn(output_size)\n",
        "        self.cur_input = None\n",
        "        self.lambda_coef = lambda_coef\n",
        "        self.normalize_l2_penalty = normalize_l2_penalty\n",
        "        self.parameters = [self.w, self.b]\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.cur_input = x\n",
        "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
        "        dw = gradient[:, :, None] @ self.cur_input[:, None, :] / self.cur_input.shape[0]\n",
        "        # Add the gradient of the L2 penalty term\n",
        "        dw += (self.lambda_coef * self.w)\n",
        "        db = gradient\n",
        "        self.gradient = [dw, db]\n",
        "        return gradient.dot(self.w)\n",
        "\n",
        "class ReLULayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.gradient is not None, \"Must call forward before backward\"\n",
        "        return gradient * self.gradient\n",
        "\n",
        "class SoftmaxOutputLayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cur_probs = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = np.clip(x, a_min=None, a_max=706) # above 709+ , exp will give -inf and high gradients of inputs will lead to mumerical instability\n",
        "        exps = np.exp(x)\n",
        "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
        "        self.cur_probs = probs\n",
        "        return probs\n",
        "\n",
        "    def backward(self, target):\n",
        "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
        "        return self.cur_probs - target\n",
        "\n",
        "class SigmoidLayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.gradient is not None, \"Must call forward before backward\"\n",
        "        return gradient * self.gradient\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "class LeakyReLULayer(NeuralNetLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.gradient = np.where(x > 0, 1.0, 0.01)\n",
        "        return self.leaky_relu(x)\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        assert self.gradient is not None, \"Must call forward before backward\"\n",
        "        return gradient * self.gradient\n",
        "\n",
        "    def leaky_relu(self, z):\n",
        "        return np.maximum(0.01 * z, z)"
      ],
      "metadata": {
        "id": "mE8DTi7Sa36i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MiniBatchMLP and MiniBatchGradientDescentOptimizer classes\n",
        "class MiniBatchMLP:\n",
        "    def __init__(self, *args):\n",
        "        self.layers = args\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def n_hidden(self):\n",
        "        n_hidden_layers = 0\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, LinearLayer):\n",
        "                n_hidden_layers += 1\n",
        "        return n_hidden_layers - 1 # - the input layer\n",
        "\n",
        "    def backward(self, target):\n",
        "        for layer in self.layers[::-1]:\n",
        "            target = layer.backward(target)\n",
        "        return target\n",
        "\n",
        "    def regularization(self):\n",
        "        l2_reg = 0\n",
        "        total_params = 0\n",
        "        for layer in self.layers:\n",
        "            if layer.parameters is not None:\n",
        "                l2_reg += np.sum(np.square(layer.parameters[0]))\n",
        "                total_params += np.prod(layer.parameters[0].shape)\n",
        "\n",
        "        if self.layers[0].normalize_l2_penalty:\n",
        "            # this is done to prevent huge losses\n",
        "            # Normalize by the total number of parameters\n",
        "            return (self.layers[0].lambda_coef * l2_reg) / total_params\n",
        "        else:\n",
        "            return self.layers[0].lambda_coef * l2_reg\n",
        "\n",
        "class MiniBatchGradientDescentOptimizer:\n",
        "    def __init__(self, net, lr):\n",
        "        self.net = net\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        for layer in self.net.layers[::-1]:\n",
        "            if layer.parameters is not None:\n",
        "                self.update(layer.parameters, layer.gradient)\n",
        "\n",
        "    def update(self, params, gradient):\n",
        "        for (p, g) in zip(params, gradient):\n",
        "            p -= self.lr * g.mean(axis=0)\n"
      ],
      "metadata": {
        "id": "_zNJSLv7bAcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming y_train is a numpy array\n",
        "unique_labels = np.unique(y_train)\n",
        "num_classes = len(unique_labels) + 1\n",
        "print(\"Number of unique labels:\", num_classes)\n",
        "\n",
        "# Create a dictionary to map unique labels to integers\n",
        "label_to_int = {label: i for i, label in enumerate(range(num_classes))}\n",
        "\n",
        "# Convert labels to integers using the dictionary\n",
        "int_labels = np.array([label_to_int[label] for label in y_train])\n",
        "\n",
        "# Create one-hot encoded labels using the correct size\n",
        "labels = np.eye(num_classes)[int_labels]\n",
        "\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "CMHfn1FmbQe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define the training loop function `train_mlp` for handling mini-batches in training the MLP model. It begins by preprocessing the data, including converting labels to integers using a dictionary mapping and one-hot encoding for classification tasks. The training loop iteratively updates the MLP's parameters using mini-batch gradient descent optimization (`MiniBatchGradientDescentOptimizer`). Within each training step, the data is shuffled, and batches are processed to compute predictions, calculate the loss (cross-entropy), perform backpropagation, and update the model's parameters. Training progress is monitored through printing epoch-wise information such as loss, training accuracy, validation accuracy, and test accuracy. The loop also stores losses and accuracies for visualization and analysis."
      ],
      "metadata": {
        "id": "jrTiYHDH6q2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop function to handle mini-batches\n",
        "\n",
        "unique_labels = np.unique(y_train)\n",
        "num_classes = len(unique_labels) + 1\n",
        "print(\"Number of unique labels:\", num_classes)\n",
        "\n",
        "# Create a dictionary to map unique labels to integers\n",
        "label_to_int = {label: i for i, label in enumerate(range(num_classes))}\n",
        "\n",
        "# Convert labels to integers using the dictionary\n",
        "int_labels = np.array([label_to_int[label] for label in y_train])\n",
        "\n",
        "### Version of train_mlp that I have been using\n",
        "\n",
        "def train_mlp(mlp, hidden_unit, data_x, data_y, x_val, y_val, x_test, y_test, steps=100, batch_size=128):\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    train_predictions = []\n",
        "    val_predictions = []\n",
        "    test_predictions = []\n",
        "\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    # labels = np.eye(num_classes)[int_labels]\n",
        "    # print('Here num class: {}'.format(labels.shape))\n",
        "\n",
        "    for units in [hidden_unit]:\n",
        "\n",
        "        print(f\"Training MLP with {mlp.n_hidden()} hidden layers and {units} hidden units...\")\n",
        "\n",
        "        opt = MiniBatchGradientDescentOptimizer(mlp, lr=1e-3)\n",
        "        for step in tqdm(range(steps)):\n",
        "            # Shuffle data for each epoch\n",
        "            inputs, labels = shuffle(data_x, data_y, random_state=42)\n",
        "            batch_losses = []\n",
        "            for batch_start in range(0, len(data_x), batch_size):\n",
        "                batch_end = batch_start + batch_size\n",
        "                x_batch = inputs[batch_start:batch_end]\n",
        "                y_batch = labels[batch_start:batch_end]\n",
        "\n",
        "                # One-hot encoding of labels\n",
        "                y_batch = np.eye(num_classes)[y_batch]\n",
        "                # forward pass\n",
        "                predictions_batch = mlp.forward(x_batch)\n",
        "                # batch training loss\n",
        "                # print(predictions_batch)\n",
        "                loss_batch = -(y_batch * np.log(predictions_batch + 1e-5)).sum(axis=-1).mean() + mlp.regularization()  # Cross Entropy\n",
        "                batch_losses.append(loss_batch)\n",
        "                # backward pass\n",
        "                mlp.backward(y_batch)\n",
        "                # propagate error back & update parameters\n",
        "                opt.step()\n",
        "\n",
        "            step_loss = np.array(batch_losses).mean()\n",
        "            losses.append(step_loss)\n",
        "\n",
        "            # Store predictions for later accuracy calculation\n",
        "            train_pred, val_pred, test_pred = np.argmax(mlp.forward(data_x), axis=1), np.argmax(mlp.forward(x_val), axis=1), np.argmax(mlp.forward(x_test), axis=1)\n",
        "\n",
        "            N_train = len(data_y)\n",
        "            N_val = len(y_val)\n",
        "            N_test = len(y_test)\n",
        "\n",
        "            accuracy_train = (data_y == train_pred).sum() / N_train\n",
        "            accuracy_eval = (y_val == val_pred).sum() / N_val\n",
        "            accuracy_test = (y_test == test_pred).sum() / N_test\n",
        "\n",
        "            print('Epoch: {} - Train Loss: {}'.format(step + 1, step_loss))\n",
        "            print('Epoch: {} - Train Accuracy: {}'.format(step + 1, accuracy_train))\n",
        "            print('Epoch: {} - Val Accuracy: {}'.format(step + 1, accuracy_eval))\n",
        "            print('Epoch: {} - Test Accuracy: {}'.format(step + 1, accuracy_test))\n",
        "\n",
        "            train_acc.append(accuracy_train)\n",
        "            val_acc.append(accuracy_eval)\n",
        "            test_acc.append(accuracy_test)\n",
        "\n",
        "    return losses, train_acc, val_acc, test_acc # train_predictions, val_predictions, test_predictions"
      ],
      "metadata": {
        "id": "Iq0Tc4DmbJDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "## Task 3.1\n",
        "\n",
        "For this task we created three separate instances of the MLP model: One with no hidden layer, one with a single hidden layer, and one with two hidden layers.  For the models with hidden layers, the ReLU activation function is used.  All versions use a softmax function for output classification. The results are stored in a DataFrame and saved to CSV files for further analysis."
      ],
      "metadata": {
        "id": "5lOhNTPxbdbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models and their configurations\n",
        "\n",
        "hidden_units = [32, 64, 128, 256]\n",
        "batch_sizes = [2, 16, 32, 64]\n",
        "\n",
        "num_input = 784\n",
        "num_output = 25\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "\n",
        "    for hidden_unit in hidden_units:\n",
        "\n",
        "        mlp_no_hidden = MiniBatchMLP(\n",
        "            LinearLayer(num_input, num_output),\n",
        "            SoftmaxOutputLayer()\n",
        "        )\n",
        "        mlp_single_hidden = MiniBatchMLP(\n",
        "            LinearLayer(num_input, hidden_unit),\n",
        "            ReLULayer(),\n",
        "            LinearLayer(hidden_unit,num_output),\n",
        "            SoftmaxOutputLayer()\n",
        "        )\n",
        "        mlp_two_hidden = MiniBatchMLP(\n",
        "            LinearLayer(num_input, hidden_unit),\n",
        "            LeakyReLULayer(),\n",
        "            LinearLayer(hidden_unit, hidden_unit),\n",
        "            LeakyReLULayer(),\n",
        "            LinearLayer(hidden_unit, num_output),\n",
        "            SoftmaxOutputLayer()\n",
        "        )\n",
        "\n",
        "        mlps = [mlp_no_hidden, mlp_single_hidden, mlp_two_hidden]\n",
        "\n",
        "        # Train the models\n",
        "        for mlp in mlps:\n",
        "            losses, train_accuracies, val_accuracies, test_accuracies = train_mlp(mlp, hidden_unit, x_train, y_train, x_val, y_val, x_test, y_test, batch_size=batch_size)\n",
        "            frame = pd.DataFrame()\n",
        "            frame['loss'] = losses\n",
        "            frame['train_acc'] = train_accuracies\n",
        "            frame['val_acc'] = val_accuracies\n",
        "            frame['test_acc'] = test_accuracies\n",
        "            frame.to_csv('../results/mlp_accuracies_{}_hidden_{}_{}.csv'.format(mlp.n_hidden(), hidden_unit, batch_size), index=False)"
      ],
      "metadata": {
        "id": "wm8T66lNdOMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2\n",
        "\n",
        "For this task, we take our two layer model from the Task 3.1 and create two new implementations, one with sigmoid activation functions and one with Leaky-ReLU activation functions. Task 3.1 established a best batch_size = [32] which will be used for task 3.3 as well. The results are stored in a DataFrame and saved to CSV files for further analysis."
      ],
      "metadata": {
        "id": "BjqjioFBdVEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_size in [32]:\n",
        "    for hidden_unit in hidden_units:\n",
        "\n",
        "        leaky_mlp_two_hidden = MiniBatchMLP(\n",
        "            LinearLayer(num_input, hidden_unit),\n",
        "            LeakyReLULayer(),\n",
        "            LinearLayer(hidden_unit, hidden_unit),\n",
        "            LeakyReLULayer(),\n",
        "            LinearLayer(hidden_unit, num_output),\n",
        "            SoftmaxOutputLayer()\n",
        "        )\n",
        "\n",
        "        sigmoid_mlp_two_hidden = MiniBatchMLP(\n",
        "                    LinearLayer(num_input, hidden_unit),\n",
        "                    SigmoidLayer(),\n",
        "                    LinearLayer(hidden_unit, hidden_unit),\n",
        "                    SigmoidLayer(),\n",
        "                    LinearLayer(hidden_unit, num_output),\n",
        "                    SoftmaxOutputLayer()\n",
        "                )\n",
        "\n",
        "        names = ['leaky', 'sigmoid']\n",
        "        # Train the models\n",
        "        for mlp, name in zip([leaky_mlp_two_hidden, sigmoid_mlp_two_hidden], names):\n",
        "            losses, train_accuracies, val_accuracies, test_accuracies = train_mlp(mlp_two_hidden, hidden_unit, x_train, y_train, x_val, y_val, x_test, y_test, batch_size=batch_size)\n",
        "            frame = pd.DataFrame()\n",
        "            frame['loss'] = losses\n",
        "            frame['train_acc'] = train_accuracies\n",
        "            frame['val_acc'] = val_accuracies\n",
        "            frame['test_acc'] = test_accuracies\n",
        "            frame.to_csv('../results/{}_mlp_accuracies_{}_hidden_{}_{}.csv'.format(name, mlp_two_hidden.n_hidden(), hidden_unit, batch_size), index=False)"
      ],
      "metadata": {
        "id": "NRQqnGXCeP3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.3\n",
        "\n",
        "For this task we implement the two hidden layer model using ReLU activation.  We implement the training function to loop over various values for the weight penalty hyperparameter (lambda_coef).  We train both with and without L2 regularization to examine how the weight penalties interact with it. The results are stored in a DataFrame and saved to CSV files for further analysis."
      ],
      "metadata": {
        "id": "1ucb-MRmebj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yugMfJCtY_JV"
      },
      "outputs": [],
      "source": [
        "# Establish list of lambda values to test over\n",
        "lambdas = [1e-4, 1e-5, 1e-3, 0.1, 0.3, 0.5, 0.7]\n",
        "\n",
        "for batch_size in [32]:\n",
        "\n",
        "    for hidden_unit in hidden_units:\n",
        "\n",
        "        for lambda_ in lambdas:\n",
        "\n",
        "            regularize = [False, True]\n",
        "\n",
        "            for regularize_ in regularize:\n",
        "\n",
        "                relu_mlp_two_hidden = MiniBatchMLP(\n",
        "                    LinearLayer(num_input, hidden_unit, lambda_, regularize_),\n",
        "                    ReLULayer(),\n",
        "                    LinearLayer(hidden_unit, hidden_unit, lambda_, regularize_),\n",
        "                    ReLULayer(),\n",
        "                    LinearLayer(hidden_unit, num_output, lambda_, regularize_),\n",
        "                    SoftmaxOutputLayer()\n",
        "                )\n",
        "\n",
        "                losses, train_accuracies, val_accuracies, test_accuracies = train_mlp(relu_mlp_two_hidden, hidden_unit, x_train, y_train, x_val, y_val, x_test, y_test, batch_size=batch_size)\n",
        "                frame = pd.DataFrame()\n",
        "                frame['loss'] = losses\n",
        "                frame['train_acc'] = train_accuracies\n",
        "                frame['val_acc'] = val_accuracies\n",
        "                frame['test_acc'] = test_accuracies\n",
        "                frame.to_csv('../results/{}_mlp_accuracies_{}_hidden_{}_{}_reg_{}.csv'.format(lambda_, relu_mlp_two_hidden.n_hidden(), hidden_unit, batch_size, regularize_), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.4\n",
        "<!--\n",
        "For this task we design a Convolutional Neural Network (CNN) using tools from the PyTorch library.  It is designed with three convolutional layers and two fully-connected (dense) layers.  We implement the CNN using the adaptive moment estimation (Adam) optimization method with cross entropy loss across a wide variety of dense layer sizes within different batch sizes to test for an ideal configuration. -->"
      ],
      "metadata": {
        "id": "Ieg9347gf6SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task we design a Convolutional Neural Network (CNN) using PyTorch. The CNN architecture comprises three convolutional layers and two fully-connected (dense) layers. The optimization is performed using the Adam optimizer along with cross-entropy loss. The code conducts experiments with different batch sizes and dense layer sizes to find an optimal configuration.\n",
        "\n",
        "We begin by importing necessary libraries and defining the CNN architecture. The BasicCNN class initializes the model with specified parameters such as dense size, number of filters, kernel size, and image size. The CustomDataset class is used to preprocess and load the dataset for training, validation, and testing.\n",
        "\n",
        "The training process involves iterating over various batch sizes and dense layer sizes. Within each iteration, the model is trained using the train_model function, which performs forward and backward passes, updates model weights, and computes training, validation, and test metrics. The results are stored in a DataFrame and saved to CSV files for further analysis."
      ],
      "metadata": {
        "id": "_FWB0dA20P18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Tuple, List, Dict, Any\n",
        "from numpy import array\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "num_gpus = [i for i in range(torch.cuda.device_count())]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self, dense_size, channels=1, n_filters=32, kernel_size=4,\n",
        "                 num_classes=25, image_size=28):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        \"\"\"\n",
        "        n_filters: number of filters\n",
        "        dense_size: size of the fully connected layer\n",
        "        kernel_size: kernel filter size\n",
        "        num_classes: # of output classes\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv2d(channels, n_filters, kernel_size=kernel_size)\n",
        "        self.conv2 = nn.Conv2d(n_filters, n_filters, kernel_size=kernel_size)\n",
        "        self.conv3 = nn.Conv2d(n_filters, n_filters, kernel_size=kernel_size)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dense = nn.Linear(n_filters * ((image_size) // 2 + (kernel_size + 1)) * ((image_size) // 2 +  (kernel_size + 1)), dense_size)\n",
        "        self.classifier = nn.Linear(dense_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.transpose(x, 1, 3) # transpose (bs, img_size, img_size, channels) --> (bs, channels, img_size, img_size)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.dense(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        # self.transform = transform\n",
        "        self.data = dataframe.drop(['label'], axis=1).values\n",
        "        self.labels = dataframe['label'].values\n",
        "        self.normalize_dataset()\n",
        "\n",
        "    def normalize_dataset(self):\n",
        "        self.data = self.data.reshape(-1, 28, 28, 1)\n",
        "        self.data = self.data / 255\n",
        "        self.data = self.data.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = torch.tensor(self.data[i])\n",
        "        y = torch.tensor(self.labels[i])\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def train_model(model: BasicCNN, dataloaders: Dict,\n",
        "                batch_size: int, criterion: torch.nn.CrossEntropyLoss,\n",
        "                optimizer: torch.optim.Optimizer, num_epochs: int, lr: float,\n",
        "                device: torch.device, dense_size: int) -> Tuple[List, List, List, List, float, float]:\n",
        "\n",
        "    if not os.path.exists('../models'):\n",
        "        os.mkdir('../models')\n",
        "\n",
        "    best_loss = 1000\n",
        "    train_loss, train_acc, eval_loss, eval_acc = [], [], [], []\n",
        "    path = os.path.join('../models/basic_cnn_{}_{}_{}_{}.pt'.format(batch_size, num_epochs, lr, dense_size))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc.item())\n",
        "            else:\n",
        "                eval_loss.append(epoch_loss)\n",
        "                eval_acc.append(epoch_acc.item())\n",
        "\n",
        "            print('{} loss: {:.4f} acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                # selecting the weights with lowest eval loss\n",
        "                best_loss = epoch_loss\n",
        "                torch.save(model.state_dict(), path)\n",
        "\n",
        "    checkpoints = torch.load(path)\n",
        "    model.load_state_dict(checkpoints)\n",
        "\n",
        "    test_loss, test_acc = test_model(model, dataloaders, criterion, device)\n",
        "\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Test loss: {:4f}'.format(test_loss))\n",
        "    print('Test Accuracy: {:4f}'.format(test_acc))\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return train_loss, train_acc, eval_loss, eval_acc, test_loss, test_acc\n",
        "\n",
        "def test_model(model: BasicCNN, dataloaders: Dict,\n",
        "               criterion: torch.nn.CrossEntropyLoss,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    loss = 0.0\n",
        "    corrects = 0.0\n",
        "    for inputs, labels in dataloaders['test']:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "        loss += loss.item() * inputs.size(0)\n",
        "        corrects += torch.sum(preds == labels.data)\n",
        "    test_loss = loss / len(dataloaders['test'].dataset)\n",
        "    test_acc = corrects.double() / len(dataloaders['test'].dataset)\n",
        "    return test_loss.item(), test_acc.item()\n",
        "\n",
        "def build_model(dense_size: int, number_of_gpus: List, device: torch.device) -> BasicCNN:\n",
        "    model = BasicCNN(dense_size)\n",
        "    if len(number_of_gpus) > 1:\n",
        "        print(\"Let's use\", len(number_of_gpus), \"GPUs!\")\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(x) for x in number_of_gpus)\n",
        "        model = torch.nn.DataParallel(model, device_ids=number_of_gpus)\n",
        "        model = model.module\n",
        "    model = model.to(device)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "88MamQbphHN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Slightly different loading and handling of training/testing/validation data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils import *\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import gdown\n",
        "\n",
        "def download_datasets() -> None:\n",
        "\n",
        "    train_data_path = \"https://drive.google.com/uc?id=1Q6Iefx7rWFSpHhuQJ7ygPHHTTxMwLYth\"\n",
        "    test_data_path = \"https://drive.google.com/uc?id=1M6aIcBXmRKRR1go9kKpDA9aGq_Uy0mz0\"\n",
        "\n",
        "    output_train = '../data/sign_mnist_train.csv'\n",
        "    output_test = '../data/sign_mnist_test.csv'\n",
        "\n",
        "    gdown.download(train_data_path, output_train)\n",
        "    gdown.download(test_data_path, output_test)\n",
        "\n",
        "train_path = '../data/sign_mnist_train.csv'\n",
        "test_path = '../data/sign_mnist_test.csv'\n",
        "\n",
        "if not os.path.exists(train_path):\n",
        "    download_datasets()\n",
        "\n",
        "train_frame = pd.read_csv(train_path)\n",
        "test_frame = pd.read_csv(test_path)\n",
        "\n",
        "real_train_frame, valid_frame = train_test_split(train_frame, test_size=0.1, random_state=42)\n",
        "# valid would be used to select the best weights\n",
        "\n",
        "train_dataset = CustomDataset(real_train_frame)\n",
        "valid_dataset = CustomDataset(valid_frame)\n",
        "test_dataset = CustomDataset(test_frame)\n",
        "\n",
        "\n",
        "def run_training() -> None:\n",
        "    batch_sizes = [2, 4, 8, 16, 32, 64, 128, 256, 1024, 2048, 4096]\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "\n",
        "        num_epochs, lr = 100, 1e-4\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "        valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "        p, l_squared = 0.5, 0.5\n",
        "        weight_decay = ((1-p)*l_squared)/len(train_dataset)\n",
        "\n",
        "        experiment_dataloader_dict = {'train': train_dataloader, 'val': valid_dataloader,\n",
        "                                    'test': test_dataloader}\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        train_losses, train_accs, eval_losses, eval_accs, test_losses, test_accs = [], [], [], [], [], []\n",
        "        dense_sizes = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
        "\n",
        "        for dense_size in dense_sizes:\n",
        "            print('============== Experiment for bs: {} and dense_size: {} =============='.format(batch_size, dense_size))\n",
        "            model = build_model(dense_size, num_gpus, device=device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "            train_loss, train_acc, eval_loss, eval_acc, test_loss, test_acc = train_model(model, experiment_dataloader_dict, optimizer=optimizer,\n",
        "                    batch_size=batch_size, criterion=criterion, num_epochs=num_epochs, lr=lr,\n",
        "                    device=device, dense_size=dense_size)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            eval_losses.append(eval_loss)\n",
        "            test_losses.append(test_loss)\n",
        "\n",
        "            train_accs.append(train_acc)\n",
        "            eval_accs.append(eval_acc)\n",
        "            test_accs.append(test_acc)\n",
        "\n",
        "        results_frame = pd.DataFrame()\n",
        "        results_frame['dense_sizes'] = dense_sizes\n",
        "        results_frame['train_loss'] = train_losses\n",
        "        results_frame['train_acc'] = train_accs\n",
        "\n",
        "        results_frame['eval_loss'] = eval_losses\n",
        "        results_frame['eval_acc'] = eval_accs\n",
        "\n",
        "        results_frame['test_loss'] = test_losses\n",
        "        results_frame['test_acc'] = test_accs\n",
        "\n",
        "        results_frame.to_csv('../results/cnn_experiments_bs_{}.csv'.format(batch_size), index=False)\n",
        "\n",
        "run_training()"
      ],
      "metadata": {
        "id": "-DnKkqpGf-2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}